{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.84432202,  0.20850925,  0.46823094],\n",
       "       [ 4.09935419, -0.13332384, -0.49268129],\n",
       "       [-1.70050345, -3.04924012,  0.76869479],\n",
       "       [-2.21379239, -3.06986255, -0.66041977],\n",
       "       [-2.102115  ,  3.36337224,  0.55345806]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "#테스트할 데이터\n",
    "X=np.array([[1,1,1,1,1,1,1,1,1,1,3,3,3,3,3,4,5,6],\n",
    "            [1,2,1,1,1,1,1,1,1,1,3,4,3,3,3,4,5,6],\n",
    "            [3,3,3,3,3,1,1,1,1,1,1,1,1,1,1,5,4,6],\n",
    "            [3,4,3,3,3,1,2,1,1,1,1,1,1,1,1,5,4,5],\n",
    "            [1,1,1,1,1,3,3,3,3,3,1,1,1,1,1,6,4,5],\n",
    "            [1,2,1,1,1,3,3,3,2,3,1,1,1,1,1,5,4,5]])\n",
    "#주성분 분석(3개의 주성분으로 축소)\n",
    "pca = PCA(n_components = 3)\n",
    "X2D = pca.fit_transform(X)\n",
    "X2D[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54\n",
      "0.42\n",
      "0.02\n"
     ]
    }
   ],
   "source": [
    "#주성분1은 54%, 주성분2는 42%, 주성분3은 2%, 나머지가 2% (주성분 1,2만으로 96%를 설명함)\n",
    "#각 주성분의 축에 해당하는 데이터셋의 분산 비율\n",
    "for i in pca.explained_variance_ratio_:\n",
    "    print('{:.2f}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.01080967, 1.1237144 , 1.01080967, 1.01080967, 1.01080967,\n",
       "       1.04652366, 0.87642192, 1.04652366, 1.15635772, 1.04652366,\n",
       "       2.94266667, 3.3355072 , 2.94266667, 2.94266667, 2.94266667,\n",
       "       4.16176255, 4.97133333, 6.14683992])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3개의 주성분으로 축소된 자료를 원래의 형태로 복원(정보의 손실이 있음)\n",
    "X3D_inv = pca.inverse_transform(X2D)\n",
    "X3D_inv[0]\n",
    "#복원 단계에서 정보 손실이 발생할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015189685531841413"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#재구성 오차(reconstruction error) 계산 : 원본 데이터와 압축 후 원복한 데이터 사이의 평균 제곱 거리\n",
    "1 - pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# MNIST 압축\n",
    "from keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 28x28 => 784로 변환\n",
    "X_train = X_train.reshape(60000, 784).astype('float32') / 255.0\n",
    "X_test = X_test.reshape(10000, 784).astype('float32') / 255.0\n",
    "print(y_train[:5])\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
       "       0.07058824, 0.49411765, 0.53333336, 0.6862745 , 0.10196079,\n",
       "       0.6509804 , 1.        , 0.96862745, 0.49803922, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.11764706, 0.14117648, 0.36862746, 0.6039216 ,\n",
       "       0.6666667 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.88235295, 0.6745098 , 0.99215686, 0.9490196 ,\n",
       "       0.7647059 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.19215687, 0.93333334,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.9843137 , 0.3647059 ,\n",
       "       0.32156864, 0.32156864, 0.21960784, 0.15294118, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.07058824, 0.85882354, 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.7137255 ,\n",
       "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.3137255 , 0.6117647 , 0.41960785, 0.99215686, 0.99215686,\n",
       "       0.8039216 , 0.04313726, 0.        , 0.16862746, 0.6039216 ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
       "       0.00392157, 0.6039216 , 0.99215686, 0.3529412 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.54509807,\n",
       "       0.99215686, 0.74509805, 0.00784314, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.04313726, 0.74509805, 0.99215686,\n",
       "       0.27450982, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.13725491, 0.94509804, 0.88235295, 0.627451  ,\n",
       "       0.42352942, 0.00392157, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.31764707, 0.9411765 , 0.99215686, 0.99215686, 0.46666667,\n",
       "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.1764706 ,\n",
       "       0.7294118 , 0.99215686, 0.99215686, 0.5882353 , 0.10588235,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.0627451 , 0.3647059 ,\n",
       "       0.9882353 , 0.99215686, 0.73333335, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.9764706 , 0.99215686,\n",
       "       0.9764706 , 0.2509804 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.18039216, 0.50980395,\n",
       "       0.7176471 , 0.99215686, 0.99215686, 0.8117647 , 0.00784314,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
       "       0.5803922 , 0.8980392 , 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.98039216, 0.7137255 , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.09411765, 0.44705883, 0.8666667 , 0.99215686, 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.7882353 , 0.30588236, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.09019608, 0.25882354, 0.8352941 , 0.99215686,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.7764706 , 0.31764707,\n",
       "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.07058824, 0.67058825, 0.85882354,\n",
       "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.7647059 ,\n",
       "       0.3137255 , 0.03529412, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.21568628, 0.6745098 ,\n",
       "       0.8862745 , 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.95686275, 0.52156866, 0.04313726, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.53333336, 0.99215686, 0.99215686, 0.99215686,\n",
       "       0.83137256, 0.5294118 , 0.5176471 , 0.0627451 , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        ], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#적절한 차원의 수 선택\n",
    "# 분산을 95%로 유지하는 차원의 수 계산\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d\n",
    "#154"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "0.9501964\n"
     ]
    }
   ],
   "source": [
    "#분산비율을 직접 지정하는 방식\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "print(pca.n_components_) #차원(주성분의 수)\n",
    "print(np.sum(pca.explained_variance_ratio_)) #분산비율 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1d707af8a00>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#변수의 수 직접 지정\n",
    "#154 차원으로 압축\n",
    "pca = PCA(n_components=154)\n",
    "X_reduced = pca.fit_transform(X_train)\n",
    "\n",
    "#784 차원으로 복원\n",
    "import matplotlib.pyplot as plt\n",
    "X_recovered = pca.inverse_transform(X_reduced)\n",
    "plt.imshow(X_recovered.reshape(60000,28,28)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...................................................................................................."
     ]
    }
   ],
   "source": [
    "# 대량의 데이터의 경우 pca를 구현하기 위해 전체 데이터셋을 메모리에 올리는 것이 어려울 수 있음\n",
    "# 점진적 pca(Incremental PCA) 알고리즘을 사용하여 미치배치 방법으로 pca를 실행할 수 있음\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "#미니배치에 해당하는 부분만 사용하므로 메모리가 절약됨\n",
    "for X_batch in np.array_split(X_train, n_batches):\n",
    "    print(\".\", end=\"\")\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "X_reduced = inc_pca.transform(X_train)\n",
    "X_recovered_inc_pca = inc_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#일반 PCA와 점진적 PCA로 MNIST 데이터를 변환한 결과 비교\n",
    "# 평균이 같은지 확인\n",
    "# allclose() 두 배열이 오차범위 내에서 같으면 True, 다르면 False\n",
    "np.allclose(pca.mean_, inc_pca.mean_)\n",
    "# True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.13066266, 0.1306604774024101)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pca.mean_), np.mean(inc_pca.mean_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랜덤 포레스트 모형\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rnd_clf = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9469"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정확도 출력\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = rnd_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "# 94.92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA를 사용하여 분산이 95%가 되도록 차원 축소\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#랜덤 포레스트 모형에 압축된 데이터 입력\n",
    "rnd_clf2 = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rnd_clf2.fit(X_train_reduced, y_train)\n",
    "#학습 시간: 8.7초(느려짐)\n",
    "#차원 축소가 반드시 학습 시간 단축을 의미하지는 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8955"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#검증용 데이터셋으로 평가\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "y_pred = rnd_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)\n",
    "# 90.09%\n",
    "#차원 축소로 인한 정보 손실로 성능이 감소되는 것이 일반적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multinomial: 소프트맥스 방식의 로지스틱 회귀분석\n",
    "#시간이 많이 걸림\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_clf = LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=0)\n",
    "log_clf.fit(X_train, y_train)\n",
    "#17.05초, 학습 시간은 더 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)\n",
    "#92.22%(랜덤포레스트보다는 성능이 좋음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, multi_class='multinomial', random_state=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#주성분분석으로 축소된 데이터셋으로 학습\n",
    "log_clf2 = LogisticRegression(multi_class=\"multinomial\", max_iter=1000, random_state=0)\n",
    "log_clf2.fit(X_train_reduced, y_train)\n",
    "#10.8초, 차원 축소 후 시간이 감소함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9233"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = log_clf2.predict(X_test_reduced)\n",
    "accuracy_score(y_test, y_pred)\n",
    "#92.03% , 성능이 약간 감소되었으나 속도가 향상되었음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
