{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #20가지 주제의 뉴스 데이터\n",
    "# import pandas as pd\n",
    "# from sklearn.datasets import fetch_20newsgroups\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "# documents = dataset.data\n",
    "# len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# documents[0] #첫번째 뉴스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# #뉴스 카테고리\n",
    "# print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "f = open(\"c:/data/text/news2.txt\")\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun\\AppData\\Local\\Temp\\ipykernel_11664\\183726181.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'recent days boss russian private military company wagner seems have gone into social media meltdown flooding telegram channel other accounts with ever more outrageous provocative statements'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':lines})\n",
    "# 알파벳 이외의 문자 제거\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "\n",
    "# 길이가 3이하인 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# 소문자 변환\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['recent', 'days', 'boss', 'russian', 'private', 'military', 'company', 'wagner', 'seems', 'gone', 'social', 'media', 'meltdown', 'flooding', 'telegram', 'channel', 'accounts', 'ever', 'outrageous', 'provocative', 'statements']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# 토큰화\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "# 불용어 제거\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "print(tokenized_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'recent days boss russian private military company wagner seems gone social media meltdown flooding telegram channel accounts ever outrageous provocative statements'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 행렬을 만들기 위해 다시 역토큰화\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "news_df['clean_doc'] = detokenized_doc\n",
    "news_df['clean_doc'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42, 329)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#상위 1000개의 단어만 처리\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features= 1000)\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape # TF-IDF 행렬의 크기 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "# 행렬 특이값 분해, 11314개의 행을 20개로 축소, n_components 토픽수\n",
    "svd_model = TruncatedSVD(n_components=20)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 329)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#토픽수 x 단어수\n",
    "np.shape(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00986489,  0.00986489,  0.0544601 , ...,  0.00986489,\n",
       "         0.00986489,  0.01699437],\n",
       "       [ 0.00789978,  0.00789978, -0.03864047, ...,  0.00789978,\n",
       "         0.00789978, -0.00108374],\n",
       "       [ 0.02477931,  0.02477931, -0.03909673, ...,  0.02477931,\n",
       "         0.02477931,  0.01170998],\n",
       "       ...,\n",
       "       [-0.00840577, -0.00840577, -0.07185177, ..., -0.00840577,\n",
       "        -0.00840577,  0.02290138],\n",
       "       [ 0.00922385,  0.00922385, -0.02704128, ...,  0.00922385,\n",
       "         0.00922385, -0.01892924],\n",
       "       [ 0.01470905,  0.01470905,  0.0249326 , ...,  0.01470905,\n",
       "         0.01470905, -0.03592365]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('prigozhin', 0.29893), ('military', 0.28873), ('russia', 0.22883), ('putin', 0.18869), ('russian', 0.18743)]\n",
      "Topic 2: [('putin', 0.43483), ('politically', 0.2426), ('expendable', 0.2426), ('prigozhin', 0.15352), ('kremlin', 0.13493)]\n",
      "Topic 3: [('shells', 0.39404), ('save', 0.15476), ('warehouses', 0.15284), ('receive', 0.13317), ('grandpa', 0.13317)]\n",
      "Topic 4: [('military', 0.23153), ('public', 0.21814), ('leadership', 0.19707), ('criticism', 0.1641), ('continued', 0.15931)]\n",
      "Topic 5: [('expendable', 0.46448), ('politically', 0.46448), ('star', 0.20358), ('political', 0.19516), ('shells', 0.11824)]\n",
      "Topic 6: [('star', 0.33274), ('political', 0.30897), ('putin', 0.16767), ('social', 0.08527), ('media', 0.08527)]\n",
      "Topic 7: [('star', 0.32048), ('victory', 0.2912), ('political', 0.29098), ('flanks', 0.26678), ('today', 0.18174)]\n",
      "Topic 8: [('exactly', 0.2349), ('political', 0.19349), ('driving', 0.17819), ('simply', 0.17819), ('tether', 0.17819)]\n",
      "Topic 9: [('observers', 0.29281), ('online', 0.29281), ('open', 0.29281), ('disloyalty', 0.29281), ('tantrums', 0.29281)]\n",
      "Topic 10: [('spending', 0.19645), ('exactly', 0.19387), ('lines', 0.18215), ('driving', 0.18215), ('defenestration', 0.18215)]\n",
      "Topic 11: [('battles', 0.30505), ('coming', 0.30505), ('rage', 0.30505), ('theory', 0.30505), ('tested', 0.30505)]\n",
      "Topic 12: [('save', 0.26031), ('shells', 0.18506), ('lying', 0.17354), ('kill', 0.17354), ('people', 0.17354)]\n",
      "Topic 13: [('flanks', 0.18705), ('according', 0.14281), ('abandoned', 0.14281), ('blood', 0.14281), ('western', 0.14281)]\n",
      "Topic 14: [('save', 0.15615), ('russian', 0.15145), ('alexey', 0.12141), ('begged', 0.12141), ('extreme', 0.12141)]\n",
      "Topic 15: [('tantrums', 0.13705), ('open', 0.13705), ('disloyalty', 0.13705), ('online', 0.13705), ('observers', 0.13705)]\n",
      "Topic 16: [('troops', 0.15019), ('fighters', 0.12751), ('battles', 0.12731), ('theory', 0.12731), ('tested', 0.12731)]\n",
      "Topic 17: [('week', 0.20347), ('celebrations', 0.18148), ('criticisms', 0.18148), ('brass', 0.18148), ('earlier', 0.18148)]\n",
      "Topic 18: [('continued', 0.32176), ('moscow', 0.32176), ('criticism', 0.25943), ('public', 0.25238), ('star', 0.22511)]\n",
      "Topic 19: [('happen', 0.2265), ('signal', 0.2265), ('politically', 0.17278), ('expendable', 0.17278), ('vertical', 0.11499)]\n",
      "Topic 20: [('shells', 0.32256), ('grandpa', 0.23051), ('receive', 0.23051), ('telegram', 0.12622), ('star', 0.11788)]\n"
     ]
    }
   ],
   "source": [
    "# 단어 집합, 1000개의 단어\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "#20개의 뉴스그룹별로 추출한 토픽 리스트 출력\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1),\n",
    "            [(feature_names[i], \n",
    "                topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_,terms)\n",
    "#각 토픽의 핵심 키워드 추출\n",
    "#LSA: 쉽고 빠르게 구현이 가능하지만 새로운 데이터가 추가되면 처음부터 다시 계산을 해야 하는 단점이 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
