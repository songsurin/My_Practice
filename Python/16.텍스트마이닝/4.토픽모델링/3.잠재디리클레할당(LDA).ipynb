{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# 영어 단어의 어근만 추출\n",
    "stm = PorterStemmer()\n",
    "\n",
    "# 영어 단어의 불용어 집합\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "# 특수문자를 제거하기 위한 정규식\n",
    "# 첫글자는 알파벳으로 시작하고 그 뒤에 영문자 대소문자, 숫자,-,_, .만 허용\n",
    "# * 0회 이상 매칭, + 1회 이상 매칭\n",
    "pattern = re.compile('[a-zA-Z][-_a-zA-Z0-9.]*')\n",
    "# 문장을 단어 단위로 분리하고 불용어 및 특수문자 제거 후 어근만 추출하여 list로 반환\n",
    "def tokenize(sentence):\n",
    "    def stem(w):\n",
    "        try: return stm.stem(w)\n",
    "        except: return w\n",
    "    #소문자로 바꾼 후 단어 구분, 불용어 제거, 패턴에 맞는 단어만 선택\n",
    "    return [stem(w) for w in word_tokenize(sentence.lower())\n",
    "            if w not in stopwords and pattern.match(w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 20\n",
      "Total words: 35\n",
      "['sugar', 'price', 'world', 'weather', 'cocoa', 'around', 'product', 'accord', 'ms', 'say', 'would', 'sweet', 'last', 'peyron', 'market', 'cost', 'gome', 'cane', 'mean', 'candi', 'increas', 'sinc', 'suppli', 'size', 'alreadi', 'first', 'biggest', 'confectioneri', 'trade', 'look', 'industri', 'whole', 'realli', 'sour', 'contain', 'make', 'need', 'compani', 'pass', 'higher', 'point', 'start', 'provid', 'consum', 'tonn', 'war', 'ukrain', 'expect', 'concern', 'el', 'ni챰o', 'climat', 'produc', 'yield', 'lower', 'come', 'year', 'europ', 'also', 'impact', 'major', 'fertilis', 'stand', 'wast', 'pack', 'highest', 'water', 'beet', 'minut', 'late', 'appoint', 'mind-bogglingli', 'expans', 'floor', 'space', 'ism', 'cologn', 'giant', 'annual', 'snack', 'fair', 'germani', 'beaten', 'herald', 'event', 'deal', 'introduct', 'tax', 'new', 'issu', 'front', 'centr', 'rocket', 'amongst', 'brightli', 'lit', 'vy', 'attent', 'one', 'brand', 'name', 'toxic', 'extra', 'sold', 'made', 'like', 'mini', 'drum', 'word', 'hayley', 'manag', 'us-bas', 'dynam', 'purpos', 'suppos', 'someth', 'almost', 'aim', 'children', 'sensit', 'convers', 'manufactur', 'get', 'still', 'compar', 'past', 'thing', 'want', 'huge', 'kid', 'longer', 'afford', 'allow', 'small', 'amount', 'within', 'anoth', 'option', 'decreas', 'especi', 'uk', 'big', 'request', 'smaller', 'tri', 'varieti', 'enjoy', 'numer', 'way', 'buy', 'quot', 'commod', 'use', 'benchmark', 'indic', 'end', 'april', 'raw', 'cent', 'per', 'pound', 'meanwhil', 'hover', 'multitud', 'factor', 'behind', 'rise', 'bad', 'diana', 'analyst', 'research', 'group', 'bloomberg', 'intellig', 'surg', 'recent', 'shortfal', 'term', 'risk', 'drive', 'inflationari', 'pressur', 'focus', 'grow', 'meteorologist', 'experi', 'latest', 'phenomenon', 'caus', 'warm', 'western', 'pacif', 'ocean', 'move', 'eastward', 'result', 'much', 'level', 'rainfal', 'south', 'america', 'brazil', 'reduc', 'disrupt', 'harvest', 'sucros', 'content', 'plant', 'tip', 'august', 'india', 'second-largest', 'seen', 'due', 'excess', 'rain', 'remaind', 'affect', 'dri', 'summer', 'drought', 'hurt', 'crop', 'psycholog', 'barrier', 'driven', 'well', 'fact', 'ivori', 'coast', 'ghana', 'two', 'cocoa-produc', 'countri', 'struggl', 'expens', 'russia', 'export', 'ad', 'farmer', 'eventu', 'therefor', 'combin', 'global', 'wage', 'inflat', 'area', 'chain', 'difficult', 'choic']\n"
     ]
    }
   ],
   "source": [
    "import tomotopy as tp\n",
    "# LDAModel 생성\n",
    "# LDA(Latent Dirichlet allocation, 잠재 디리클레 할당)\n",
    "# 주어진 문서에 대하여 각 문서에 어떤 주제들이 존재하는지를 서술하는 확률적 토픽 모델 기법\n",
    "# 토픽의 개수(k) 20개, 5회 미만 등장한 단어들은 제거\n",
    "model = tp.LDAModel(k=20, min_cf=5)\n",
    "\n",
    "# 파일에서 한 줄씩 읽어와서 model에 추가\n",
    "for i, line in enumerate(open('c:/data/text/english_news.txt', encoding='ms949')):\n",
    "    model.add_doc(tokenize(line)) # 공백 기준으로 단어를 나누어 model에 추가\n",
    "\n",
    "# train(0) : 0회 학습, model의 num_words, num_vocabs 값을 확인하기 위해\n",
    "# 실제로 학습은 하지 않고 학습 준비만 하는 상태\n",
    "model.train(0)\n",
    "print('Total docs:', len(model.docs))\n",
    "print('Total words:', model.num_words)\n",
    "print(model.vocabs) #단어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mr.',\n",
       " 'emma',\n",
       " 'could',\n",
       " 'would',\n",
       " 'mrs.',\n",
       " 'miss',\n",
       " 'must',\n",
       " 'harriet',\n",
       " 'much',\n",
       " 'said',\n",
       " 'think',\n",
       " 'thing',\n",
       " 'one',\n",
       " 'weston',\n",
       " 'everi',\n",
       " 'elton',\n",
       " 'say',\n",
       " 'know',\n",
       " 'knightley',\n",
       " 'well',\n",
       " 'littl',\n",
       " 'never',\n",
       " 'might',\n",
       " 'good',\n",
       " 'woodhous',\n",
       " 'time',\n",
       " 'jane',\n",
       " 'quit',\n",
       " 'look',\n",
       " 'like']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.vocabs)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0\tsugar, price, world, weather, cocoa\n",
      "Topic #1\tsugar, price, world, weather, cocoa\n",
      "Topic #2\tsugar, price, world, weather, cocoa\n",
      "Topic #3\tsugar, price, world, weather, cocoa\n",
      "Topic #4\tsugar, price, world, weather, cocoa\n",
      "Topic #5\tsugar, price, world, weather, cocoa\n",
      "Topic #6\tsugar, price, world, weather, cocoa\n",
      "Topic #7\tcocoa, sugar, price, world, weather\n",
      "Topic #8\tsugar, price, world, weather, cocoa\n",
      "Topic #9\tsugar, price, world, weather, cocoa\n",
      "Topic #10\tsugar, price, world, weather, cocoa\n",
      "Topic #11\tsugar, price, world, weather, cocoa\n",
      "Topic #12\tweather, sugar, price, world, cocoa\n",
      "Topic #13\tsugar, price, world, weather, cocoa\n",
      "Topic #14\tsugar, price, world, weather, cocoa\n",
      "Topic #15\tsugar, price, world, weather, cocoa\n",
      "Topic #16\tprice, sugar, world, weather, cocoa\n",
      "Topic #17\tworld, sugar, price, weather, cocoa\n",
      "Topic #18\tsugar, price, world, weather, cocoa\n",
      "Topic #19\tsugar, price, world, weather, cocoa\n"
     ]
    }
   ],
   "source": [
    "#200회 학습\n",
    "model.train(200)\n",
    "# 학습된 토픽들을 출력\n",
    "for i in range(model.k):\n",
    "    # 0~19번 토픽별 상위 단어 10개 추출\n",
    "    res = model.get_topic_words(i, top_n=10)\n",
    "    print(f'Topic #{i}', end='\\t')\n",
    "    print(', '.join(w for w, p in res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'volum',\n",
       " 'chapter',\n",
       " 'emma',\n",
       " 'woodhous',\n",
       " 'handsom',\n",
       " 'clever',\n",
       " 'rich',\n",
       " 'comfort',\n",
       " 'home',\n",
       " 'happi',\n",
       " 'disposit',\n",
       " 'seem',\n",
       " 'unit',\n",
       " 'best',\n",
       " 'bless',\n",
       " 'exist',\n",
       " 'live',\n",
       " 'nearli',\n",
       " 'twenty-on',\n",
       " 'year',\n",
       " 'world',\n",
       " 'littl',\n",
       " 'distress',\n",
       " 'vex',\n",
       " 'youngest',\n",
       " 'two',\n",
       " 'daughter',\n",
       " 'affection',\n",
       " 'indulg',\n",
       " 'father',\n",
       " 'consequ',\n",
       " 'sister',\n",
       " 'marriag',\n",
       " 'mistress',\n",
       " 'hous',\n",
       " 'earli',\n",
       " 'period',\n",
       " 'mother',\n",
       " 'die',\n",
       " 'long',\n",
       " 'ago',\n",
       " 'indistinct',\n",
       " 'remembr',\n",
       " 'caress',\n",
       " 'place',\n",
       " 'suppli',\n",
       " 'excel',\n",
       " 'woman',\n",
       " 'gover',\n",
       " 'fallen',\n",
       " 'littl',\n",
       " 'short',\n",
       " 'mother',\n",
       " 'affect',\n",
       " 'sixteen',\n",
       " 'year',\n",
       " 'miss',\n",
       " 'taylor',\n",
       " 'mr.',\n",
       " 'woodhous',\n",
       " 'famili',\n",
       " 'less',\n",
       " 'gover',\n",
       " 'friend',\n",
       " 'fond',\n",
       " 'daughter',\n",
       " 'particularli',\n",
       " 'emma',\n",
       " 'intimaci',\n",
       " 'sister',\n",
       " 'even',\n",
       " 'miss',\n",
       " 'taylor',\n",
       " 'ceas',\n",
       " 'hold',\n",
       " 'nomin',\n",
       " 'offic',\n",
       " 'gover',\n",
       " 'mild',\n",
       " 'temper',\n",
       " 'hardli',\n",
       " 'allow',\n",
       " 'impos',\n",
       " 'restraint',\n",
       " 'shadow',\n",
       " 'author',\n",
       " 'long',\n",
       " 'pass',\n",
       " 'away',\n",
       " 'live',\n",
       " 'togeth',\n",
       " 'friend',\n",
       " 'friend',\n",
       " 'mutual',\n",
       " 'attach',\n",
       " 'emma',\n",
       " 'like']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "emma_raw=nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "tokenize(emma_raw)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total docs: 1\n",
      "Total words: 67220\n",
      "['mr.', 'emma', 'could', 'would', 'mrs.', 'miss', 'must', 'harriet', 'much', 'said', 'think', 'thing', 'one', 'weston', 'everi', 'elton', 'say', 'know', 'knightley', 'well', 'littl', 'never', 'might', 'good', 'woodhous', 'time', 'jane', 'quit', 'look', 'like', 'come', 'see', 'thought', 'great', 'feel', 'friend', 'noth', 'go', 'dear', 'alway', 'man', 'fairfax', 'even', 'churchil', 'soon', 'give', 'may', 'make', 'wish', 'shall', 'without', 'hope', 'seem', 'day', 'want', 'frank', 'first', 'sure', 'father', 'made', 'happi', 'inde', 'bodi', 'ever', 'oh', 'young', 'talk', 'two', 'though', 'better', 'way', 'love', 'hartfield', 'upon', 'long', 'walk', 'speak', 'realli', 'take', 'believ', 'rather', 'letter', 'bate', 'us', 'word', 'done', 'mani', 'marri', 'away', 'mean', 'poor', 'appear', 'hear', 'home', 'howev', 'mind', 'moment', 'woman', 'last', 'manner']\n"
     ]
    }
   ],
   "source": [
    "model=tp.LDAModel(k=5, min_cf=5) #토픽모델링 함수\n",
    "model.add_doc(tokenize(emma_raw)) #단어별로 나누고 모형에 추가\n",
    "model.train(0) # 0 학습횟수, 아직 학습이 시작된 상태가 아님\n",
    "print('Total docs:',len(model.docs))\n",
    "print('Total words:',model.num_words)\n",
    "print(list(model.vocabs)[:100]) #단어 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('would', 0.042939528822898865), ('must', 0.027141273021697998)]\n",
      "Topic #0\twould, must\n",
      "[('elton', 0.03026706725358963), ('think', 0.025825826451182365)]\n",
      "Topic #1\telton, think\n",
      "[('could', 0.051300812512636185), ('mrs.', 0.031094225123524666)]\n",
      "Topic #2\tcould, mrs.\n",
      "[('know', 0.03233422338962555), ('said', 0.02356864884495735)]\n",
      "Topic #3\tknow, said\n",
      "[('emma', 0.053037822246551514), ('mr.', 0.03622720018029213)]\n",
      "Topic #4\temma, mr.\n"
     ]
    }
   ],
   "source": [
    "model.train(100) #100회 학습\n",
    "for i in range(model.k):\n",
    "    res=model.get_topic_words(i,top_n=2) #토픽의 상위단어 2개\n",
    "    print(res)\n",
    "    print(f'Topic #{i}',end='\\t')\n",
    "    print(', '.join(w for w,p in res))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
