{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "boston=load_boston()\n",
    "dfX=pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "dfy=pd.DataFrame(boston.target, columns=[\"MEDV\"])\n",
    "df=pd.concat([dfX, dfy], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습용과 검증용을 7:3으로 구분\n",
    "N=len(df)\n",
    "ratio=0.7\n",
    "np.random.seed(0)\n",
    "idx_train=np.random.choice(np.arange(N), np.int64(ratio*N),replace=False)\n",
    "idx_test=list(set(np.arange(N)).difference(idx_train))\n",
    "            # 집합               차집합     학습용 70%\n",
    "df_train=df.iloc[idx_train]\n",
    "df_test=df.iloc[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'boston_house_prices.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   MEDV   R-squared:                       0.728\n",
      "Model:                            OLS   Adj. R-squared:                  0.718\n",
      "Method:                 Least Squares   F-statistic:                     70.06\n",
      "Date:                Thu, 09 Mar 2023   Prob (F-statistic):           8.57e-88\n",
      "Time:                        11:05:12   Log-Likelihood:                -1043.0\n",
      "No. Observations:                 354   AIC:                             2114.\n",
      "Df Residuals:                     340   BIC:                             2168.\n",
      "Df Model:                          13                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     35.0719      5.932      5.913      0.000      23.404      46.739\n",
      "CRIM          -0.1005      0.035     -2.869      0.004      -0.169      -0.032\n",
      "ZN             0.0381      0.017      2.248      0.025       0.005       0.071\n",
      "INDUS          0.0202      0.072      0.281      0.779      -0.121       0.162\n",
      "CHAS           1.1498      1.020      1.127      0.260      -0.856       3.156\n",
      "NOX          -17.3942      4.522     -3.847      0.000     -26.288      -8.500\n",
      "RM             3.8640      0.485      7.959      0.000       2.909       4.819\n",
      "AGE            0.0004      0.016      0.023      0.982      -0.030       0.031\n",
      "DIS           -1.3285      0.236     -5.626      0.000      -1.793      -0.864\n",
      "RAD            0.3741      0.084      4.447      0.000       0.209       0.540\n",
      "TAX           -0.0160      0.005     -3.315      0.001      -0.025      -0.007\n",
      "PTRATIO       -0.8989      0.153     -5.885      0.000      -1.199      -0.598\n",
      "B              0.0095      0.003      3.015      0.003       0.003       0.016\n",
      "LSTAT         -0.5013      0.060     -8.423      0.000      -0.618      -0.384\n",
      "==============================================================================\n",
      "Omnibus:                      136.641   Durbin-Watson:                   1.998\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              602.833\n",
      "Skew:                           1.618   Prob(JB):                    1.25e-131\n",
      "Kurtosis:                       8.513   Cond. No.                     1.47e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.47e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "model = sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train)\n",
    "result = model.fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7519796502601109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#검증용 데이터셋으로 모형 평가\n",
    "pred=result.predict(df_test)\n",
    "# Residual Sum of Square(잔차의 분산, 오차의 크기)\n",
    "ssr=((df_test.MEDV-pred) ** 2).sum()\n",
    "# Total Sum of Square(종속변수 y의 분산)\n",
    "sst=((df_test.MEDV-df_test.MEDV.mean())**2).sum()\n",
    "# 결정계수: 모형의 설명력(0~1 사이의 값)\n",
    "rsquared=1-ssr/sst\n",
    "rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 14), (152, 14))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#학습용,검증용으로 구분\n",
    "df_train, df_test=train_test_split(df, test_size=0.3, random_state=0)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (354, 1), (152, 13), (152, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습용X,y 검증용 X,y로 구분\n",
    "dfX_train, dfX_test, dfy_train, dfy_test=train_test_split(dfX, dfy, test_size=0.3, random_state=0)\n",
    "dfX_train.shape, dfy_train.shape, dfX_test.shape, dfy_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습용 R2=0.773, 검증용 R2=0.589\n",
      "학습용 R2=0.729, 검증용 R2=0.778\n",
      "학습용 R2=0.749, 검증용 R2=0.668\n",
      "학습용 R2=0.757, 검증용 R2=0.668\n",
      "학습용 R2=0.705, 검증용 R2=0.840\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "scores=np.zeros(5)\n",
    "cv=KFold(5, shuffle=True, random_state=0)\n",
    "for i, (idx_train, idx_test) in enumerate(cv.split(df)):\n",
    "    df_train=df.iloc[idx_train]\n",
    "    df_test=df.iloc[idx_test]\n",
    "\n",
    "    model=sm.OLS.from_formula(\"MEDV~\"+\"+\".join(boston.feature_names), data=df_train)\n",
    "    result=model.fit()\n",
    "\n",
    "    pred=result.predict(df_test)\n",
    "    ssr=((df_test.MEDV-pred)**2).sum()\n",
    "    sst=((df_test.MEDV-df_test.MEDV.mean())**2).sum()\n",
    "    rsquared=1-ssr/sst\n",
    "\n",
    "    scores[i]=rsquared\n",
    "    print(f\"학습용 R2={result.rsquared:.3f}, 검증용 R2={rsquared:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.58922238 0.77799144 0.66791979 0.6680163  0.83953317]\n",
      "[33.44898    18.65881615 21.23463289 29.22251557 16.57369039]\n",
      "[3.84290922 3.38979394 3.07473854 3.6463452  3.03058651]\n",
      "0.7085366175182802\n",
      "23.82772699990077\n",
      "3.396874681157459\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error\n",
    "scores1=np.zeros(5)\n",
    "scores2=np.zeros(5)\n",
    "scores3=np.zeros(5)\n",
    "cv=KFold(5, shuffle=True, random_state=0)\n",
    "for i, (idx_train, idx_test) in enumerate(cv.split(df)):\n",
    " df_train=df.iloc[idx_train]\n",
    " df_test=df.iloc[idx_test]\n",
    "\n",
    " model=sm.OLS.from_formula(\"MEDV ~ \" + \"+\".join(boston.feature_names), data=df_train)\n",
    " result=model.fit()\n",
    "\n",
    " pred=result.predict(df_test)\n",
    " #결정계수를 구하는 함수\n",
    " rsquared=r2_score(df_test.MEDV, pred)\n",
    " scores1[i]=rsquared\n",
    " #평균제곱오차(Mean Squared Error): 오차의 제곱의 합계의 평균값\n",
    " mse=mean_squared_error(df_test.MEDV, pred)\n",
    " scores2[i]=mse\n",
    " #평균절대오차(Mean Absolute Error): 오차의 절대값 합계의 평균값\n",
    " mae=mean_absolute_error(df_test.MEDV, pred)\n",
    " scores3[i]=mae\n",
    "\n",
    "print(scores1) # rsquare\n",
    "print(scores2) # mse\n",
    "print(scores3) # mae\n",
    "print(np.mean(scores1))\n",
    "print(np.mean(scores2))\n",
    "print(np.mean(scores3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "class StatsmodelsOLS(BaseEstimator, RegressorMixin):\n",
    " def __init__(self, formula):\n",
    "   self.formula=formula\n",
    "   self.model=None\n",
    "   self.data=None\n",
    "   self.result=None\n",
    "\n",
    " def fit(self, dfX, dfy):\n",
    "   self.data=pd.concat([dfX, dfy], axis=1)\n",
    "   self.model=smf.ols(self.formula, data=self.data)\n",
    "   self.result=self.model.fit()\n",
    "\n",
    " def predict(self, new_data):\n",
    "   return self.result.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.78350932, 4.31958518, 4.60810513, 5.40578538, 4.07107976])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "model=StatsmodelsOLS(\"MEDV ~ \" + \"+\".join(boston.feature_names))\n",
    "cv=KFold(5, shuffle=True, random_state=0)\n",
    "cross_val_score(model, dfX, dfy, scoring=\"r2\", cv=cv)\n",
    "\n",
    "#평균제곱오차로 평가하는 경우\n",
    "result=cross_val_score(model, dfX, dfy, scoring='neg_mean_squared_error', cv=cv)\n",
    "\n",
    "#음수로 나온 결과값을 양수로 변환\n",
    "rmse_score=np.sqrt(-result)\n",
    "rmse_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
